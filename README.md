# Bias Detection in Federated Learning and Swarm Learning


## Results
### Federated Learning

The identification of bias in federated learning is an essential element in upholding principles of fairness and ethical considerations within decentralised machine learning systems. Federated learning, a technique that entails training models across numerous decentralised clients while maintaining data localization, is vulnerable to both data bias and algorithmic bias. 

In order to identify bias, a range of fairness metrics, including differential impact, demographic parity, and equal opportunity, are utilised to evaluate whether model predictions exhibit preferential treatment towards specific demographic groups. 

The process of detecting bias encompasses several key components, namely data analysis, model monitoring, adherence to fairness constraints, bias audits, and implementation of mitigation measures. Throughout this process, utmost consideration is given to maintaining data privacy constraints. 

The necessity of client collaboration is paramount, with openness and accountability serving as crucial factors in upholding equity. The aforementioned procedure follows an iterative approach, wherein models are periodically reassessed for bias in order to ensure the provision of fair and dependable artificial intelligence solutions within federated learning settings.

<img width="400" alt="Screenshot 2023-09-25 at 11 40 24 PM" src="https://github.com/noopur-zambare/Bias-Detection/assets/92505473/3eefb4dc-5658-4de3-8f4d-188ed405111d">

### Swarm Learning

The identification of bias in swarm learning is of utmost importance in upholding principles of fairness and ethical considerations inside decentralised machine learning systems, wherein autonomous agents work collectively to train models. 

Swarm learning, a collaborative learning paradigm, can give rise to biases stemming from imbalanced data distributions or algorithmic biases, as agents inside the system possess their own distinct data sources. 

The process of identifying bias includes the application of fairness criteria such as disparate impact, demographic parity, and equal opportunity to evaluate and address any inequities in model projections that are based on group characteristics. 

<img width="400" alt="Screenshot 2023-09-25 at 11 38 51 PM" src="https://github.com/noopur-zambare/Bias-Detection/assets/92505473/c27712fb-d801-4f25-bd33-0dc20f7b5afc">
