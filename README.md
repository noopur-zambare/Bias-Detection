# Bias Detection in Federated Learning and Swarm Learning


## Results
### Federated Learning

The identification of bias in federated learning is an essential element in upholding principles of fairness and ethical considerations within decentralised machine learning systems. Federated learning, a technique that entails training models across numerous decentralised clients while maintaining data localization, is vulnerable to both data bias and algorithmic bias. 

In order to identify bias, a range of fairness metrics, including differential impact, demographic parity, and equal opportunity, are utilised to evaluate whether model predictions exhibit preferential treatment towards specific demographic groups. 

The process of detecting bias encompasses several key components, namely data analysis, model monitoring, adherence to fairness constraints, bias audits, and implementation of mitigation measures. Throughout this process, utmost consideration is given to maintaining data privacy constraints. 

The necessity of client collaboration is paramount, with openness and accountability serving as crucial factors in upholding equity. The aforementioned procedure follows an iterative approach, wherein models are periodically reassessed for bias in order to ensure the provision of fair and dependable artificial intelligence solutions within federated learning settings.

<img width="400" alt="Screenshot 2023-09-25 at 11 40 24 PM" src="https://github.com/noopur-zambare/Bias-Detection/assets/92505473/3eefb4dc-5658-4de3-8f4d-188ed405111d">

### Differential Privacy

Differential Privacy approaches are critical in Federated Learning for increasing data privacy across decentralised devices or participants. Federated Learning models can be trained collectively while minimising the danger of disclosing sensitive individual data by employing approaches such as Laplace or Gaussian noise addition to model updates, implementing local differential privacy safeguards, and properly managing privacy budgets. These strategies enable organisations and people to leverage the collective intelligence of their data without jeopardising their users' or contributors' privacy rights, thereby building trust and compliance with data privacy regulations.


### Swarm Learning

The identification of bias in swarm learning is of utmost importance in upholding principles of fairness and ethical considerations inside decentralised machine learning systems, wherein autonomous agents work collectively to train models. 

Swarm learning, a collaborative learning paradigm, can give rise to biases stemming from imbalanced data distributions or algorithmic biases, as agents inside the system possess their own distinct data sources. 

The process of identifying bias includes the application of fairness criteria such as disparate impact, demographic parity, and equal opportunity to evaluate and address any inequities in model projections that are based on group characteristics. 

<img width="400" alt="Screenshot 2023-09-25 at 11 38 51 PM" src="https://github.com/noopur-zambare/Bias-Detection/assets/92505473/c27712fb-d801-4f25-bd33-0dc20f7b5afc">

### Decentralized Identity Management

In Swarm Learning with Decentralised Identity Management, strong security measures are essential. This entails utilising blockchain technology for safe identity verification, putting cryptographic methods into use to protect data privacy during model updates, and assuring the reliability of smart contracts regulating the learning process. To prevent unauthorised access and ensure transparency, it's imperative to include access control systems, auditing, and permission management. Multiple parties can jointly train machine learning models without jeopardising their data privacy or the process's integrity by adhering to data privacy laws, monitoring ongoingly, and preventingÂ attacks. 

